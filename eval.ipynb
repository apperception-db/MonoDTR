{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, List\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "# import fire\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "# import cupy as np\n",
    "# import cupy.typing as npt\n",
    "importlib.reload(np)\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from tqdm import tqdm\n",
    "from pyquaternion import Quaternion\n",
    "\n",
    "from aputils import Video, camera_config\n",
    "from scripts._path_init import *\n",
    "from visualDet3D.data.pipeline import build_augmentator\n",
    "from visualDet3D.networks.detectors.monodtr_detector import MonoDTR\n",
    "from visualDet3D.networks.utils.utils import BackProjection, BBox3dProjector\n",
    "from visualDet3D.utils.utils import cfg_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submodules.Yolov5_StrongSORT_OSNet.trackers.multi_tracker_zoo import create_tracker\n",
    "from submodules.Yolov5_StrongSORT_OSNet.yolov5.utils.general import scale_boxes\n",
    "from submodules.Yolov5_StrongSORT_OSNet.yolov5.utils.plots import Annotator, colors\n",
    "from submodules.Yolov5_StrongSORT_OSNet.trackers.strong_sort.strong_sort import StrongSORT\n",
    "\n",
    "print('CUDA available: {}'.format(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import monodtrutils\n",
    "importlib.reload(monodtrutils)\n",
    "from monodtrutils import NuScenesMonoDataset, Detection, format_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIR = \"./data/boston-seaport/\"\n",
    "names = ['Car', 'Pedestrian', 'Cyclist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"config/config.py\"\n",
    "gpu = 1\n",
    "checkpoint_path = \"./workdirs/MonoDTR/checkpoint/MonoDTR.pth\"\n",
    "\n",
    "\n",
    "# Read Config\n",
    "cfg: \"Any\" = cfg_from_file(config)\n",
    "\n",
    "# Force GPU selection in command line\n",
    "cfg.trainer.gpu = gpu\n",
    "torch.cuda.set_device(cfg.trainer.gpu)\n",
    "\n",
    "cfg.is_running_test_set = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create StrongSORT model\n",
    "FILE = Path('').resolve()\n",
    "reid_weights = FILE / \"weights/osnet_x0_25_msmt17.pt\"  # model.pt path\n",
    "device = torch.device(int(gpu))\n",
    "half = False\n",
    "tracker = create_tracker(\"strongsort\", reid_weights, device, half)\n",
    "tracker.model.warmup()\n",
    "print('loaded tracker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detection the model\n",
    "detector = MonoDTR(cfg.detector)\n",
    "detector = detector.cuda()\n",
    "state_dict = torch.load(checkpoint_path, map_location='cuda:{}'.format(cfg.trainer.gpu))\n",
    "new_dict = state_dict.copy()\n",
    "detector.load_state_dict(new_dict, strict=False)\n",
    "detector.eval()\n",
    "print('loaded detector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "dataset = NuScenesMonoDataset(cfg, VIDEO_DIR)\n",
    "print('constructed dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "time_load = []\n",
    "time_detect = []\n",
    "time_track = []\n",
    "with torch.no_grad():\n",
    "    detector.eval()\n",
    "    result_path = os.path.join(cfg.path.preprocessed_path, 'data')\n",
    "    if os.path.isdir(result_path):\n",
    "        os.system(\"rm -r {}\".format(result_path))\n",
    "        print(\"clean up the recorder directory of {}\".format(result_path))\n",
    "    os.mkdir(result_path)\n",
    "    print(\"rebuild {}\".format(result_path))\n",
    "\n",
    "    projector = BBox3dProjector().cuda()\n",
    "    backprojector = BackProjection().cuda()\n",
    "\n",
    "    trackings = {}\n",
    "    output = None\n",
    "    dt, seen = [0.0, 0.0, 0.0, 0.0], 0\n",
    "    curr_frame, prev_frame = None, None\n",
    "    detections: \"List[Detection]\" = []\n",
    "    trackings = []\n",
    "    for index in tqdm(range(len(dataset))): \n",
    "        t1 = time.time()\n",
    "        data = dataset[index]\n",
    "        if isinstance(data['calib'], list):\n",
    "            P2 = data['calib'][0]\n",
    "        else:\n",
    "            P2 = data['calib']\n",
    "        collated_data = dataset.collate_fn([data])\n",
    "            \n",
    "\n",
    "        # images: torch.Tensor [N=1 x 3 x h x w]\n",
    "        # P2: [np.array[3 x 4]]\n",
    "        images, P2 = collated_data\n",
    "        camera_translation = np.array(data['camera_translation'])\n",
    "        camera_rotation = Quaternion(data['camera_rotation'])\n",
    "        time_load.append(time.time() - t1)\n",
    "    \n",
    "        t1 = time.time()\n",
    "        scores, bbox, clss = detector([\n",
    "            images.cuda().float().contiguous(),\n",
    "            torch.tensor(np.array(P2)).cuda().float()\n",
    "        ])\n",
    "        # scores, bbox, obj_names = test_mono_detection(collated_data, detector, None, cfg=cfg)\n",
    "        assert scores.shape[0] == bbox.shape[0] and scores.shape[0] == clss.shape[0]\n",
    "        bbox_2d = bbox[:, 0:4]\n",
    "\n",
    "        if bbox.shape[1] <= 4:\n",
    "            raise Exception('Should run 3D')\n",
    "        bbox_3d_state = bbox[:, 4:]  # [cx,cy,z,w,h,l,alpha, bot, top]\n",
    "        P2 = P2[0]\n",
    "        bbox_3d_state_3d = backprojector(bbox_3d_state, P2)  # [x, y, z, w,h ,l, alpha, bot, top]\n",
    "\n",
    "        _, _, thetas = projector(bbox_3d_state_3d, bbox_3d_state_3d.new(P2))\n",
    "\n",
    "        original_P = data['original_P']\n",
    "        scale_x = original_P[0, 0] / P2[0, 0]\n",
    "        scale_y = original_P[1, 1] / P2[1, 1]\n",
    "        \n",
    "        shift_left = original_P[0, 2] / scale_x - P2[0, 2]\n",
    "        shift_top  = original_P[1, 2] / scale_y - P2[1, 2]\n",
    "        bbox_2d[:, 0:4:2] += shift_left\n",
    "        bbox_2d[:, 1:4:2] += shift_top\n",
    "\n",
    "        bbox_2d[:, 0:4:2] *= scale_x\n",
    "        bbox_2d[:, 1:4:2] *= scale_y\n",
    "\n",
    "        detection = format_detections(scores, bbox_2d, bbox_3d_state_3d, thetas, clss)\n",
    "        bbox_3d__ = []\n",
    "        for i, b in enumerate(detection.bbox_3d_state_3d):\n",
    "            b = b[:3]\n",
    "            b = np.array(b.cpu())\n",
    "            b = camera_rotation.rotate(b)\n",
    "            b = b + camera_translation\n",
    "            bbox_3d__.append(b)\n",
    "        detection.bbox_3d_state_3d = torch.tensor(np.array(bbox_3d__)).to(device)\n",
    "            \n",
    "        detections.append(detection)\n",
    "        time_detect.append(time.time() - t1)\n",
    "        \n",
    "        \n",
    "        t1 = time.time()\n",
    "        im = collated_data[0]\n",
    "        im0 = np.ascontiguousarray(data['original_image'].copy()[:, :, ::-1])\n",
    "        curr_frame = im0.copy()\n",
    "        annotator = Annotator(im0, line_width=2, pil=not ascii)\n",
    "        if prev_frame is not None and curr_frame is not None:\n",
    "            if hasattr(tracker, 'tracker') and hasattr(tracker.tracker, 'camera_update'):\n",
    "                tracker.tracker.camera_update(prev_frame, curr_frame)\n",
    "        det = bbox_2d\n",
    "        if scores.size()[0] == 0:\n",
    "            det = torch.tensor([])\n",
    "        else:\n",
    "            det = torch.cat([\n",
    "                bbox_2d,\n",
    "                scores.unsqueeze(1),\n",
    "                clss.unsqueeze(1),\n",
    "            ], dim=1)\n",
    "        tracking = []\n",
    "        if det is not None and len(det):\n",
    "            # Rescale boxes from img_size to im0 size\n",
    "            # det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()  # xyxy\n",
    "            outputs = tracker.update(det.cpu(), im0)\n",
    "            time_track.append(time.time() - t1)\n",
    "            if len(outputs) > 0:\n",
    "                for j, (output, conf) in enumerate(zip(outputs, det[:, 4])):\n",
    "                    bboxes = output[0:4]\n",
    "                    id = int(output[4])\n",
    "                    cls = int(output[5])\n",
    "                    index = int(output[7])\n",
    "                    if output[8] == 0:\n",
    "                        bbox_3d = bbox_3d_state_3d[index][:3]\n",
    "                        bbox_3d = camera_rotation.rotate(bbox_3d)\n",
    "                        bbox_3d = camera_translation + bbox_3d\n",
    "                        bbox_3d = bbox_3d.tolist()\n",
    "                    else:\n",
    "                        bbox_3d = None\n",
    "\n",
    "                    bbox_left = output[0]\n",
    "                    bbox_top = output[1]\n",
    "                    bbox_w = output[2] - output[0]\n",
    "                    bbox_h = output[3] - output[1]\n",
    "                    label = f'{id} {names[cls]} {conf:.2f}'\n",
    "                    annotator.box_label(bboxes, label, color=colors(cls, True))\n",
    "                    tracking.append((id, cls, index, bboxes.tolist(), bbox_3d, output[8]))\n",
    "        else:\n",
    "            time_track.append(time.time() - t1)\n",
    "        \n",
    "        trackings.append(tracking)\n",
    "        \n",
    "        im0 = annotator.result()\n",
    "        imgs.append(im0)\n",
    "        prev_frame = curr_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = 20\n",
    "h = imgs[0].shape[0]\n",
    "w = imgs[0].shape[1]\n",
    "video_writer = cv2.VideoWriter('./test.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
    "\n",
    "for im0 in imgs:\n",
    "    video_writer.write(im0[:, :, ::-1])\n",
    "\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(time_load))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(time_detect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(time_track))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('time_load.json', \"w\") as f:\n",
    "    json.dump(time_load, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tolist(l):\n",
    "    if isinstance(l, list):\n",
    "        return l\n",
    "    return l.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('detection.json', 'w') as f:\n",
    "    json.dump(\n",
    "        [\n",
    "            {\n",
    "                \"bbox\": tolist(d.bbox_2d),\n",
    "                \"clss\": tolist(d.clss),\n",
    "                \"bbox_3d\": tolist(d.bbox_3d_state_3d),\n",
    "                \"thetas\": tolist(d.thetas),\n",
    "                \"scores\": tolist(d.scores),\n",
    "            }\n",
    "            for d\n",
    "            in detections\n",
    "        ],\n",
    "        f,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trackings.json', 'w') as f:\n",
    "    json.dump(\n",
    "        [\n",
    "            [\n",
    "                {\n",
    "                    \"id\": d[0],\n",
    "                    \"cls\": d[1],\n",
    "                    \"index\": d[2],\n",
    "                    \"bbox\": d[3],\n",
    "                    \"bbox_3d\": d[4],\n",
    "                    \"detection_age\": d[5],\n",
    "                }\n",
    "                for d\n",
    "                in t\n",
    "            ]\n",
    "            for t\n",
    "            in trackings\n",
    "        ],\n",
    "        f,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackings[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "84d29d97db383f657f75dc58896aa3b3c19a4f49ccd7e64fe139d0ff912fb64a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
